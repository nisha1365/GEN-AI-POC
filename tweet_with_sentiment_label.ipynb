{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha1365/GEN-AI-POC/blob/main/tweet_with_sentiment_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = \"/content/tweets_by_topics_110_final.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlciMeFo-yAB",
        "outputId": "a3aea5ab-7cce-48d2-e6e5-53825341b50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5im16-5D81id",
        "outputId": "76f32976-3621-4314-eb86-1f353a51460d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Create a function to clean the text\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "def clean_text(text):\n",
        "    # Remove mentions (@)\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "    # Remove hashtags (#) without removing the word following it\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    # Remove URLs (http/https)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|https?\\/\\/t\\.co\\/\\S+', '', text)\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\" # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\" # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = re.sub(emoji_pattern, '', text)\n",
        "    # Remove extra white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create a new Excel writer object\n",
        "new_file_path = \"/content/tweets_with_sentiment.xlsx\"\n",
        "writer = pd.ExcelWriter(new_file_path)\n",
        "\n",
        "# Iterate through each sheet in the Excel file\n",
        "for sheet_name in xls.sheet_names:\n",
        "    # Read the sheet into a DataFrame\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "    # Add a new column for cleaned text\n",
        "    df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "    # Add a new column for sentiment label\n",
        "    df['sentiment_label'] = df['clean_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "\n",
        "    # Classify the sentiment based on the compound score\n",
        "    df['sentiment_label'] = df['sentiment_label'].apply(lambda score: 'positive' if score > 0 else ('negative' if score < 0 else 'neutral'))\n",
        "\n",
        "    # Save the modified DataFrame to the new Excel file\n",
        "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "# Close the Excel writer\n",
        "writer.save()\n",
        "\n",
        "print(\"Sentiment analysis and data cleaning completed. New file created:\", new_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wEIO2Y4_Hp9",
        "outputId": "74766075-2ca3-49cc-c380-773e2f535a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f3c25fff2f96>:26: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
            "  writer.save()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis and data cleaning completed. New file created: /content/tweets_with_sentiment.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyMmsiRA_MNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}