{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha1365/GEN-AI-POC/blob/main/Wynn_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_x1HUb8XHXc"
      },
      "outputs": [],
      "source": [
        "!pip install gradio bs4 requests Pillow tensorflow\n",
        "!pip install gradio requests beautifulsoup4 Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EG_I_Mmherc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8ULAYcNmay1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# from PIL import Image\n",
        "\n",
        "# def scrape_links_with_keywords(query, search_type='isch'):\n",
        "#     \"\"\"\n",
        "#     Scrape links from Google search based on the query.\n",
        "#     Args:\n",
        "#     - query: The search query.\n",
        "#     - search_type: 'isch' for images, 'web' for webpages.\n",
        "#     \"\"\"\n",
        "#     query = query.replace(' ', '+')\n",
        "#     if search_type == 'web':\n",
        "#         url = f\"https://www.google.com/search?q={query}\"\n",
        "#     else:\n",
        "#         url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
        "#     response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "#     links = []\n",
        "#     if search_type == 'web':\n",
        "#         for g in soup.find_all('a'):\n",
        "#             href = g.get('href')\n",
        "#             if href and \"url?q=\" in href and not \"webcache\" in href:\n",
        "#                 clean_link = href.split(\"url?q=\")[1].split(\"&sa=U\")[0]\n",
        "#                 if clean_link and \"http\" in clean_link:\n",
        "#                     links.append(clean_link)\n",
        "#     else:\n",
        "#         for img in soup.find_all('img'):\n",
        "#             src = img.get('src')\n",
        "#             if src and \"http\" in src:\n",
        "#                 links.append(src)\n",
        "\n",
        "#     return links[:5]\n",
        "\n",
        "# def process_data(name, keywords):\n",
        "#     query = f\"{name} {keywords}\"\n",
        "\n",
        "#     # Fetch webpage links\n",
        "#     web_links = scrape_links_with_keywords(query, search_type='web')\n",
        "#     # Fetch image links\n",
        "#     image_links = scrape_links_with_keywords(query)\n",
        "\n",
        "#     web_results = []\n",
        "#     for link in web_links:\n",
        "#         score = calculate_keyword_score(link, keywords)\n",
        "#         web_results.append(f\"Web Link: {link}, Score: {score}\")\n",
        "\n",
        "#     image_results = \"\\n\".join(image_links)\n",
        "\n",
        "#     return \"\\n\".join(web_results), image_results\n",
        "\n",
        "# def calculate_keyword_score(link, keywords):\n",
        "#     # Simplified: score based on how many keywords appear in the link\n",
        "#     score = sum(keyword.lower() in link.lower() for keyword in keywords.split())\n",
        "#     return score\n",
        "\n",
        "# # Adjust the Gradio interface to include two output boxes\n",
        "# iface = gr.Interface(fn=process_data,\n",
        "#                      inputs=[\"text\", \"text\"],\n",
        "#                      outputs=[\"text\", \"text\"],\n",
        "#                      description=\"Enter a name and keywords. The system will return the top 5 Google webpage links related to the name and keywords, along with a basic score indicating the relevance of the link to the keywords; and the top 5 Google Image links related to the name and keywords.\")\n",
        "\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "wCicga_Gerrk",
        "outputId": "c9917227-c4ca-4967-b86a-a8335ec53f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e3157de69e7a0c2184.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e3157de69e7a0c2184.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NE1YeRi1eryT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_links_with_keywords(query, search_type='isch'):\n",
        "    \"\"\"\n",
        "    Scrape links from Google search based on the query, excluding specific patterns.\n",
        "    \"\"\"\n",
        "    query = query.replace(' ', '+')\n",
        "    if search_type == 'web':\n",
        "        url = f\"https://www.google.com/search?q={query}\"\n",
        "    else:\n",
        "        url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
        "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    links = []\n",
        "    if search_type == 'web':\n",
        "        for g in soup.find_all('a'):\n",
        "            href = g.get('href')\n",
        "            if href and \"url?q=\" in href and not \"webcache\" in href:\n",
        "                clean_link = href.split(\"url?q=\")[1].split(\"&sa=U\")[0]\n",
        "                # Check if the link contains unwanted patterns\n",
        "                if clean_link and \"http\" in clean_link and not any(unwanted in clean_link for unwanted in [\"maps.google.com\", \"support.google.com\"]):\n",
        "                    links.append(clean_link)\n",
        "    else:\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src')\n",
        "            if src and \"http\" in src:\n",
        "                links.append(src)\n",
        "\n",
        "    return links[:5]\n",
        "\n",
        "def process_data(name, keywords):\n",
        "    query = f\"{name} {keywords}\"\n",
        "    # Fetch webpage links\n",
        "    web_links = scrape_links_with_keywords(query, search_type='web')\n",
        "    # Format web links as HTML for clickable output\n",
        "    web_results = \"<br>\".join([f'<a href=\"{link}\" target=\"_blank\">{link}</a>' for link in web_links])\n",
        "\n",
        "    # Fetch image links\n",
        "    image_links = scrape_links_with_keywords(query)\n",
        "\n",
        "    return web_results, image_links\n",
        "\n",
        "# Adjust Gradio interface to include HTML output for clickable links\n",
        "iface = gr.Interface(fn=process_data,\n",
        "                     inputs=[gr.Textbox(label=\"Name\"), gr.Textbox(label=\"Keywords\")],\n",
        "                     outputs=[gr.HTML(label=\"Webpage Links\"), gr.Gallery(label=\"Images\")],\n",
        "                     description=\"Enter a name and keywords. The system will return the top 5 Google webpage links related to the name and keywords; and display the top 5 Google Image links related to the name and keywords.\")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "LXAGANVK0mWM",
        "outputId": "360edec1-c379-4a7d-80b8-4c74d17fc42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9750ea2b855791f6ca.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9750ea2b855791f6ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_links_with_keywords(query, search_type='isch'):\n",
        "    \"\"\"\n",
        "    Scrape links from Google search based on the query, excluding specific patterns.\n",
        "    \"\"\"\n",
        "    query = query.replace(' ', '+')\n",
        "    if search_type == 'web':\n",
        "        url = f\"https://www.google.com/search?q={query}\"\n",
        "    else:\n",
        "        url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
        "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    links = []\n",
        "    if search_type == 'web':\n",
        "        for g in soup.find_all('a'):\n",
        "            href = g.get('href')\n",
        "            if href and \"url?q=\" in href and not \"webcache\" in href:\n",
        "                clean_link = href.split(\"url?q=\")[1].split(\"&sa=U\")[0]\n",
        "                # Check if the link contains unwanted patterns\n",
        "                if clean_link and \"http\" in clean_link and not any(unwanted in clean_link for unwanted in [\"maps.google.com\", \"support.google.com\"]):\n",
        "                    links.append(clean_link)\n",
        "    else:\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src')\n",
        "            if src and \"http\" in src:\n",
        "                links.append(src)\n",
        "\n",
        "    return links[:10]\n",
        "\n",
        "def process_data(name, keywords):\n",
        "    query = f\"{name} {keywords}\"\n",
        "    # Fetch webpage links\n",
        "    web_links = scrape_links_with_keywords(query, search_type='web')\n",
        "    # Format web links as HTML for clickable output\n",
        "    web_results = \"<br>\".join([f'<a href=\"{link}\" target=\"_blank\">{link}</a>' for link in web_links])\n",
        "    # Fetch image links for the top web links\n",
        "    image_links = []\n",
        "    for link in web_links:\n",
        "        # Scrape images from each web link\n",
        "        try:\n",
        "            response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            for img in soup.find_all('img'):\n",
        "                src = img.get('src')\n",
        "                if src:\n",
        "                    image_links.append(src)\n",
        "        except:\n",
        "            pass\n",
        "    return web_results, image_links\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Adjust Gradio interface to include HTML output for clickable links\n",
        "iface = gr.Interface(fn=process_data,\n",
        "                     inputs=[gr.Textbox(label=\"Name\"), gr.Textbox(label=\"Keywords\")],\n",
        "                     outputs=[gr.HTML(label=\"Webpage Links\"), gr.Gallery(label=\"Images\")],\n",
        "                     description=\"Enter a name and keywords. The system will return the top 5 Google webpage links related to the name and keywords; and display the top 5 Google Image links related to the name and keywords.\")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Av2vR2XjTbM2",
        "outputId": "7dd6f44d-d35a-4108-cb54-2ed10584d2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a0089dfa198165e5ff.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a0089dfa198165e5ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top URLs with Scrapped Images."
      ],
      "metadata": {
        "id": "MVIpbLvta5U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible that some web pages will not allow the images to be scraped, or might require authentication or interaction with the user before allowing access to the images. In such cases, you might see errors or empty image galleries. In some cases, you might be able to work around the issue by providing authentication credentials or interacting with the page programmatically using techniques like web automation or headless browsing. However, these techniques are beyond the scope of what can be implemented within this Gradio interface."
      ],
      "metadata": {
        "id": "zNi25TDta2q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere tiktoken"
      ],
      "metadata": {
        "id": "a_KhdC4Jde2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8c5b30-8b9a-4253-aef4-f7e0d8143ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.3.3-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.0)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.20,>=0.19->cohere) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (24.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.1)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, tiktoken, cohere\n",
            "Successfully installed cohere-5.3.3 fastavro-1.9.4 httpx-sse-0.4.0 tiktoken-0.6.0 types-requests-2.31.0.20240406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "ue2E4kukduWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "AWTk319udfDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "189ab4d4-243d-4f45-ad85-c95e7d6ca9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_type = \"azure\"\n",
        "openai.api_key = \"1b9938c280974153ad1e7cabc7580f11\"\n",
        "openai.api_base = \"https://cognizantaimlopenaidemoaccount.openai.azure.com/\"\n",
        "openai.api_version = \"2023-05-15\""
      ],
      "metadata": {
        "id": "DDqn1nALdfSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_webpage(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)  # Added a timeout for the request\n",
        "        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:  # This will catch any request-related errors\n",
        "        return f\"Error scraping {url}: {e}\"  # Returns the error instead of text\n",
        "\n",
        "# Example usage\n",
        "urls = [\n",
        "    \"https://www.google.com/preferences%3Fhl%3Dzh-TW\",\n",
        "     \"https://www.capitol.hawaii.gov/memberpage.aspx%3Fmember%3Dluke%26year%3D2020\",\n",
        "     \"https://en.wikipedia.org/wiki/Sylvia_Luke\"\n",
        "]\n",
        "\n",
        "# Concatenate text from all URLs\n",
        "all_text = ' '.join(scrape_webpage(url) for url in urls)\n",
        "print(all_text[:500])  # Print the first 500 characters to verify\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1bhOL-9dxHf",
        "outputId": "cdaa1a4e-e2e7-498a-8f33-8f30709060a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping https://www.google.com/preferences%3Fhl%3Dzh-TW: 404 Client Error: Not Found for url: https://www.google.com/preferences%3Fhl%3Dzh-TW Error scraping https://www.capitol.hawaii.gov/memberpage.aspx%3Fmember%3Dluke%26year%3D2020: 403 Client Error: Forbidden for url: https://www.capitol.hawaii.gov/memberpage.aspx%3Fmember%3Dluke%26year%3D2020 Sylvia Jung Luke (née Chang, born December 15, 1967) is an American attorney and politician who is serving as the 16th lieutenant governor of Ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Sf56J0Ztwtya",
        "outputId": "16b55330-f6c3-4a86-e59c-8fcd464d8b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Error scraping https://www.google.com/preferences%3Fhl%3Dzh-TW: 404 Client Error: Not Found for url: https://www.google.com/preferences%3Fhl%3Dzh-TW Error scraping https://www.capitol.hawaii.gov/memberpage.aspx%3Fmember%3Dluke%26year%3D2020: 403 Client Error: Forbidden for url: https://www.capitol.hawaii.gov/memberpage.aspx%3Fmember%3Dluke%26year%3D2020 Sylvia Jung Luke (née Chang, born December 15, 1967) is an American attorney and politician who is serving as the 16th lieutenant governor of Hawaii. She served as a member of the Hawaii House of Representatives for 24 years, from 1998 until her election as lieutenant governor in 2022. She is the first Korean American politician ever elected to a statewide office in the United States.[1]\\n Sylvia Luke was born Sylvia Eun Jung Chang (장은정) on December 15, 1967, in Seoul, South Korea.[2][3][4] Because her Korean name, Eun, means “silver,” her mother gave her the English name Sylvia.[5]\\n When she was nine years old, her family moved to Hawai‘i and she started school at Queen Kaahumanu Elementary School. She could speak only a few words of English and recalls how her homeroom teacher volunteered to coach her daily after school.[6] She later attended Lincoln Elementary School, Kawananakoa Middle School and graduated from Roosevelt High School in 1985.\\n While a junior and senior at Roosevelt High School, Luke conducted independent research at the University of Hawaiʻi at Mānoa on genetics in entomology. This experience led her to become a strong supporter of Early College, a program that allows high school students to take college classes and earn credits.[7]\\n She earned a Bachelor of Arts degree from the University of Hawaiʻi at Mānoa and a Juris Doctor from the University of San Francisco School of Law. She also spent a semester studying international studies at Yonsei University in Korea.[8]\\n Luke consecutively served from January 1999 until 2013 in the Hawaii House of Representatives for the 26th district, and from January 2013 until 2022 for the 25th district. In addition to serving as a legislator, she worked as an attorney in private practice.[5]\\n For 24 years, Luke served in the Hawaii House of Representatives as the representative for Makiki, Punchbowl, Nuuanu, Dowsett Highlands, Pacific Heights and Pauoa.[9]\\n She has served as Vice Chair of the House Committee on Economic Development and Business Concerns, Vice Speaker from 2001 to 2004, Chair of the House Select Committee on War Preparedness, and Chair of the House Committee on Judiciary from 2005 to 2006. As Judiciary Chair, Luke was instrumental in passing laws to require lifetime registration for the worst sex offenders and all convicted felons to submit DNA samples and hand impressions.[10]\\n Luke gained a reputation as a powerful lawmaker in her leadership of the House Finance Committee, where she asked tough questions about how state money is spent.[6] She was the first woman to chair the Finance Committee in the Legislature’s history.[11]\\n Luke, along with Senator Michelle Kidani and Representative Della Au Belatti, has been instrumental in leading efforts to maximize federal reimbursements for Medicaid. Luke estimated that the Hawaii State Department of Education could potentially claim up to $50 million to $100 million more each year. Frustrated by the lack of progress from the department in recovering these funds, a legislative working group was formed to take a more proactive approach.[12]\\n Luke took significant steps to regain control over state spending. Luke believes it is important for the Legislature to have transparency and the ability to prioritize spending, including special funds, which are dedicated accounts for specific programs. She has also tackled other complex fiscal issues, such as addressing temporary hires and introducing legislation for regular reviews of tax credits and exemptions. Luke\\'s efforts aimed to ensure accountability, effective allocation of public funds, and the Legislature\\'s ability to fulfill its responsibilities.[5]\\n During the COVID-19 pandemic, Luke worked with federal, state, and county partners to discuss how to best allocate the federal relief funds from the Coronavirus Aid, Relief, and Economic Security Act (CARES) to address public health and safety while navigating issues with balancing the state budget. Using information from the House Select Committee on COVID-19 Economic and Financial Preparedness, which worked with representatives from local and state government, private industry, and nonprofit agencies and organizations to inform the House of Representatives on the State\\'s economic and financial preparedness, funding was allocated for rent subsidies, unemployment insurance, and personal protective equipment (PPE) for schools and hospitals.[13]\\n When Hawai‘i’s unemployment rate rose from one of the lowest in the country to the highest in April 2020, Luke worked with legislative leadership to establish a satellite unemployment insurance claims center at the Convention Center, where hundreds of state employees volunteered to help process the backlog of over 160,000 unemployment claims.[13] She also helped direct federal funding to support local business recovery efforts through the Hawai‘I Restaurant Card program.[14]\\n In 2021, Governor Ige’s executive budget proposed cuts to programs like HIV prevention and treatment, tuberculosis control, prenatal care, and sexual assault treatment.[15] The COVID-19 relief funds helped Luke\\'s committee to prevent these cuts of $80 million in social services, including support for low-income families, cash support for child care, senior citizen support, and crime victim compensation.[16]\\n In 2021, Luke announced her intention to run for Lieutenant Governor of Hawaii. She stated that she would work to implement laws she helped to pass to increase affordable housing, ensure that all 3-4-year olds have access to preschool, increase broadband capacity and use unused school lands for teacher housing.[17]\\n During the race, the Be Change Now super PAC spent $1.2 million opposing Luke, and $2.9 million to support her opponent former City councilmember Ikaika Anderson, setting the state record for the most money spent by a PAC for a single race in Hawaii.[18]\\n Luke was inaugurated as the 16th Lieutenant Governor of Hawai‘i on December 5, 2022.\\n In her inauguration speech, Luke described her experiences as a working mother and how important access to preschools and child care is to families who depend on them, citing it as a cost-of-living issue.[19]\\n In January 2023, Luke launched the Ready Keiki initiative to expand preschool services statewide and build hundreds of classrooms to increase the state\\'s capacity to serve underserved children.[20] The plan includes renovating classrooms at Department of Education elementary schools, expanding state subsidies provided under the Preschool Open Doors program to low- and moderate-income families, and creating teaching classrooms at high schools and college campuses.[21]\\n Luke is working with departments across the state government to maximize the flow of federal money into Hawaiʻi’s broadband access and coordinate federal funding strategies.[22]\\n Luke plans to modernize many of the Secretary of State responsibilities and duties for the public, including name changes, international document certifications, and administrative rules processing, including a searchable database.[23][24] The state\\'s Apostille system has largely remained unchanged since 1960.[24] In 2023, House Bill 964, which passed the Legislature, would have increased the $1 fee to help fund upgrades including a new online filing system. Despite no testimony offered in opposition, Governor Green later received feedback and issued a veto which Luke agreed to.[25]\\n Luke launched the 24th Annual State Employees\\' Food Drive to support the Hawaii Foodbank. The kickoff was held at a Hawaii Rainbow Warriors baseball game for the first time in its history.[26] Luke also led the inaugural Food Drive Fest at the Hawaii State Capitol.[27]\\n Luke served as acting Governor during the start of the August 2023 fires on Hawaii Island and Maui.[28] Luke immediately declared a state of emergency and issued an emergency proclamation which activated the National Guard and authorized expenditures for disaster relief. \\n Tasked by Governor Green with facilitating essential relief for Maui residents, Luke announced a coordinated effort with Mahi Pono, Maui Economic Opportunity, and the Council for Native Hawaiian Advancement to distribute donations for those affected by the Maui fires.[29] A donation site at Maui\\'s Queen Ka\\'ahumanu Center was set-up and is led by a partnership between Lt. Governor Luke, the County of Maui, Mahi Pono, Salvation Army, and Feed My Sheep.[30]\\n Luke also worked with the Council of Native Hawaiian Advancement and the Office of Hawaiian Affairs to open a new Maui Relief Storage Facility, a 30,000 square feet warehouse to manage Oahu-based donations headed for victims of the Maui fires.[31]\\n As a Korean immigrant and the first American of Korean heritage elected to a statewide office, Luke participates in events that recognize and promote the bilateral ties between the Republic of Korea and both Hawai\\'i and the United States.\\n Luke met with the Chair of the South Korean National Assembly\\'s intelligence committee in December 2022.[32] She also spoke at an event hosted by the Consulate General of Korea, \"Korea Matters for Hawai\\'i/Hawai\\'i Matters for Korea.\"[33]\\n On August 19, 2023, the South Korean government and Consul General Lee Seo Young worked with Luke to donate $2 million USD for Maui fire disaster relief.[34]\\n She is married to Michael Luke and has one son.[35][36] The couple met in college while both were serving as senators for the Associated Students of the University of Hawaii.[13]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def summarize_text(text):\n",
        "    persona = \"You are a highly efficient summarizer.\"\n",
        "    prompt = f\"Summarize the following text:\\n\\n{text}\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"GPT4Turbo128\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": persona},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024,\n",
        "        top_p=1.0,\n",
        "        frequency_penalty=0.0,\n",
        "        presence_penalty=0.0\n",
        "    )\n",
        "    summary = response.choices[0].message['content'].strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "WgwZ2-USfLOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUWqKvnvw5H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(summary):\n",
        "    persona = \"You are a responsible ChatGPT which can generate any personal profile of a person. You will never generate any misleading and harmful content.\"\n",
        "    prompt = f\"\"\"\n",
        "    Given the summary:\n",
        "\n",
        "    {summary}\n",
        "\n",
        "    Extract the person's information in a structured dictionary format as shown in the example.\n",
        "    example:\n",
        "\n",
        "{{\n",
        "  'Name': 'virat kohli',\n",
        "  'Date of birth': '5 November 1988' ,\n",
        "  'Age': '35',\n",
        "  'Summary': 'Virat Kohli (born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team.',\n",
        "  'Occupation': 'cricketer',\n",
        "  'Date of birth': '5 November 1988' ,\n",
        "  'Insta Followers': '268 million'\n",
        "  'Hobbies':  'Workout, Travelling, Singing, Dancing',\n",
        "  'Other personal details':  'Married to Anushka',\n",
        "  'Favourite Food' : 'Chole Bhature',\n",
        "  'Favourite Colour' : 'Blue',\n",
        "  'Favourite Car': 'Audi R8',\n",
        "  'Net Worth': '128 Million Dollars',\n",
        "  'Family' : '''Father - Prem Nath Kohli,\n",
        "                Mother - Saroj Kohli,\n",
        "                Husband/Wife - Anushka Shamra'''\n",
        "\n",
        "\n",
        "\n",
        "}}\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=\"GPT4Turbo128\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": persona},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "        top_p=1.0,\n",
        "        frequency_penalty=0.0,\n",
        "        presence_penalty=0.0\n",
        "    )\n",
        "    extracted_info = response.choices[0].message['content'].strip()\n",
        "    return extracted_info\n",
        "\n"
      ],
      "metadata": {
        "id": "eeqGU8FXdxSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `all_text` is the concatenated text from all your scraped URLs\n",
        "summarized_text = summarize_text(all_text)\n",
        "extracted_information = extract_information(summarized_text)\n",
        "print(extracted_information)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvVmJGpBdxV7",
        "outputId": "f2ae5c20-1737-4458-cc62-d33965e4334f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"Name\": \"Sylvia Jung Luke\",\n",
            "  \"Date of birth\": \"December 15, 1967\",\n",
            "  \"Place of birth\": \"Seoul, South Korea\",\n",
            "  \"Occupation\": \"Attorney and Politician\",\n",
            "  \"Education\": {\n",
            "    \"Undergraduate\": \"BA from the University of Hawaiʻi at Mānoa\",\n",
            "    \"Law School\": \"Juris Doctor from the University of San Francisco School of Law\",\n",
            "    \"Study Abroad\": \"Semester at Yonsei University in Korea\"\n",
            "  },\n",
            "  \"Political Career\": {\n",
            "    \"Years in Hawaii House of Representatives\": \"1998-2022\",\n",
            "    \"Leadership Roles\": [\n",
            "      \"Vice Chair of the House Committee on Economic Development and Business Concerns\",\n",
            "      \"Vice Speaker\",\n",
            "      \"Chair of the House Committee on Judiciary\",\n",
            "      \"First woman to chair the Finance Committee\"\n",
            "    ],\n",
            "    \"Lieutenant Governor of Hawaii\": \"16th Lieutenant Governor, elected in 2022\"\n",
            "  },\n",
            "  \"Legislative Focus\": [\n",
            "    \"Maximizing federal Medicaid reimbursements\",\n",
            "    \"Improving fiscal accountability\",\n",
            "    \"Ensuring efficient allocation of public funds\",\n",
            "    \"Public health, education, and disaster relief\"\n",
            "  ],\n",
            "  \"Policy Initiatives as Lieutenant Governor\": [\n",
            "    \"Expanding preschool services through the Ready Keiki initiative\",\n",
            "    \"Modernizing the Secretary of State's office\",\n",
            "    \"Enhancing broadband access across Hawaii\"\n",
            "  ],\n",
            "  \"Disaster Response Efforts\": \"Coordinating relief for Maui fire victims\",\n",
            "  \"Achievements\": \"First Korean American elected to a statewide office in the United States\",\n",
            "  \"Significant Events\": [\n",
            "    \"COVID-19 pandemic response\",\n",
            "    \"2023 Maui fires relief efforts\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "id": "zdCor4JNmb6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8067310-854c-455f-f16e-c0be34d3df00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSv2pLJOdbqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBZGCHvfdd1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Plus Summary"
      ],
      "metadata": {
        "id": "7xu20pvo_xl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_google_search_summary(name, keywords, char_limit=250):\n",
        "    query = f\"{name} {keywords}\".replace(' ', '+')\n",
        "    url = f\"https://www.google.com/search?q={query}\"\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        return \"Failed to retrieve search results.\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    snippet = soup.find('div', class_='BNeawe s3v9rd AP7Wnd')\n",
        "\n",
        "    if snippet:\n",
        "        summary = snippet.text\n",
        "        # Trim the summary to the character limit, trying to end on a full sentence if possible.\n",
        "        if len(summary) > char_limit:\n",
        "            end = summary.rfind('. ', 0, char_limit)\n",
        "            if end != -1:\n",
        "                return summary[:end + 1]  # Include the period in the output\n",
        "            else:\n",
        "                # If there's no good stopping point, just cut off at the character limit\n",
        "                return summary[:char_limit] + \"...\"\n",
        "        else:\n",
        "            return summary\n",
        "    else:\n",
        "        return \"Summary not found.\"\n",
        "\n",
        "# Example usage\n",
        "name = \"Virat Kohli\"\n",
        "keywords = \"Cricket,India\"\n",
        "summary = fetch_google_search_summary(name, keywords)\n",
        "print(f\"Name: {name}\\nSummary: {summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lFdQPcF9bgh",
        "outputId": "7695ea98-ccc3-431a-a28c-b9cb7d5673b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Virat Kohli\n",
            "Summary: Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli]; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team. He is a right-handed batsman and an occasional medium-fast bowler.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Personal Info For the Person if available on wikipedia"
      ],
      "metadata": {
        "id": "hZ3wDNba_oMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_info_from_wikipedia_infobox(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Attempt to find any infobox\n",
        "    infobox = soup.find('table', {'class': lambda x: x and 'infobox' in x.split()})\n",
        "    if not infobox:\n",
        "        return \"Infobox not found.\"\n",
        "\n",
        "    info = {}\n",
        "    for tr in infobox.find_all('tr'):\n",
        "        header = tr.find('th')\n",
        "        value = tr.find('td')\n",
        "        if header and value:\n",
        "            key = ' '.join(header.text.split())\n",
        "            info[key] = ' '.join(value.text.split())\n",
        "\n",
        "    return info\n",
        "\n",
        "# Example usage\n",
        "url = \"https://en.wikipedia.org/wiki/Virat_Kohli\"\n",
        "personal_info = get_info_from_wikipedia_infobox(url)\n",
        "for key, value in personal_info.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE61yVaq-2hV",
        "outputId": "6dac337a-f36c-4b4e-dfcb-22d766b95ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Born: (1988-11-05) 5 November 1988 (age 35)Delhi, India\n",
            "Nickname: Cheeku[a]\n",
            "Height: 5 ft 9 in (175 cm)[2]\n",
            "Batting: Right-handed\n",
            "Bowling: Right-arm medium\n",
            "Role: Top-order batter\n",
            "Relations: Anushka Sharma (wife)\n",
            "Website: viratkohli.foundation\n",
            "National side: India (2008–present)\n",
            "Test debut (cap 269): 20 June 2011 v West Indies\n",
            "Last Test: 4 January 2024 v South Africa\n",
            "ODI debut (cap 175): 18 August 2008 v Sri Lanka\n",
            "Last ODI: 19 November 2023 v Australia\n",
            "ODI shirt no.: 18\n",
            "T20I debut (cap 31): 12 June 2010 v Zimbabwe\n",
            "Last T20I: 17 January 2024 v Afghanistan\n",
            "T20I shirt no.: 18\n",
            "2006–present: Delhi\n",
            "2008–present: Royal Challengers Bengaluru\n",
            "Competition: Competition Test ODI FC LA Matches 113 292 145 326 Runs scored 8,848 13,848 11097 15290 Batting average 49.15 58.67 50.21 57.48 100s/50s 29/30 50/72 36/38 54/80 Top score 254* 183 254* 183 Balls bowled 175 662 643 726 Wickets 0 5 3 5 Bowling average – 136.00 112.66 148.20 5 wickets in innings – 0 0 0 10 wickets in match – 0 0 0 Best bowling – 1/13 1/19 1/13 Catches/stumpings 111/– 151/– 142/– 169/–\n",
            "Matches: 113\n",
            "Runs scored: 8,848\n",
            "Batting average: 49.15\n",
            "100s/50s: 29/30\n",
            "Top score: 254*\n",
            "Balls bowled: 175\n",
            "Wickets: 0\n",
            "Bowling average: –\n",
            "5 wickets in innings: –\n",
            "10 wickets in match: –\n",
            "Best bowling: –\n",
            "Catches/stumpings: 111/–\n",
            "Medal record Men's Cricket Representing India ICC Cricket World Cup Winner 2011 India-Bangladesh-Sri Lanka Runner-up 2023 India ICC Champions Trophy Winner 2013 England and Wales Runner-up 2017 England and Wales ICC T20 World Cup Runner-up 2014 Bangladesh ICC World Test Championship Runner-up 2019–2021 Runner-up 2021–2023 ICC Under-19 Cricket World Cup Winner 2008 Malaysia ACC Asia Cup Winner 2010 Sri Lanka Winner 2016 Bangladesh Winner 2023 Pakistan-Sri Lanka: \n"
          ]
        }
      ]
    }
  ]
}